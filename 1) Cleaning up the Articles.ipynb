{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Press Data\n",
    "## 1) Cleaning up the Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "import copy\n",
    "\n",
    "import json\n",
    "import bson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://<username>:<password>@cluster.mongodb.net/pressdata_db?retryWrites=true&w=majority\")\n",
    "db = client.get_database('pressdata_db')\n",
    "\n",
    "# All the records are part of the \"clippings\" collection\n",
    "records = db.clippings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions to clean up individual articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General text clean-up function for the text of one article, where the text is stored as a list of strings\n",
    "# This includes removing unnecessary formating characters as well as combining any separated sentences or paragraphs \n",
    "# into coherent units.\n",
    "# The function also removes some unnecessary sections in the article that aren't important for the BIGS program\n",
    "# (see the remove_irrelevant_sections() function)\n",
    "# Input: a list of strings\n",
    "def cleanup_text(text):\n",
    "    \n",
    "    # Remove formatting characters: extra spaces, tabs, new lines, and \"xa0\" characters\n",
    "    remove_formatting_characters(text)\n",
    "\n",
    "    # re-formats the apostrophes in the text, and also changes any \\xa0 characters embedded in the text\n",
    "    change_apostrophes_and_xa0(text)\n",
    "    \n",
    "    # combines any separated paragraphs or sentences into one\n",
    "    combine_lines_to_sentences(text)\n",
    "\n",
    "    # removes irrelevant sections from the text (see the function below for more details)\n",
    "    remove_irrelevant_sections(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Removes all unecessary formatting characters\n",
    "# this includes spaces and tabs at the beginning and end of each line, as well as \n",
    "# lone \"\\xa0\" characters, new line characters and tabs\n",
    "# Input: a list of strings\n",
    "def remove_formatting_characters(text):\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "            text[i] = text[i].strip()\n",
    "            i += 1\n",
    "    return\n",
    "    \n",
    "# Re-formats the apostrophes in the text, and also changes any \\xa0 characters embedded in the text to spaces\n",
    "# Related to the apostrophes, this is mainly done since spaCy doesn't always recognize the apostrophes in the raw text, \n",
    "# for unknown reasons. So this re-formatting is done for greater consistency. \n",
    "# Input: list of strings\n",
    "def change_apostrophes_and_xa0(text):\n",
    "    i = 0\n",
    "    while i < len(text):        \n",
    "        text[i] = text[i].replace(\"â€™\", \"'\")\n",
    "        text[i] = text[i].replace(\"\\xa0\", \" \")\n",
    "        i += 1\n",
    "        \n",
    "    return\n",
    "\n",
    "# Combines any lines in the text list to form complete sentences and paragraphs\n",
    "# After applying this function, each line in the text list (should) corresponds to a paragraph\n",
    "# in the article\n",
    "# Input: list of strings\n",
    "def combine_lines_to_sentences(text):\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "\n",
    "        # get the current line, and the next line\n",
    "        line = text[i]\n",
    "        if i != len(text) - 1:\n",
    "            next_line = text[i+1]\n",
    "        else:\n",
    "            next_line = None\n",
    "\n",
    "        # if the current line is a blank line, remove it; otherwise, if both the current line and the next line \n",
    "        # contain useful text, they are merged together to form one paragraph or sentence\n",
    "        if line == '':\n",
    "            text.pop(i)\n",
    "            continue \n",
    "        elif (next_line != None) and (next_line != ''):\n",
    "            if next_line[0] in [\",\", \"'\"]:\n",
    "                text[i+1] = line + next_line\n",
    "            else:\n",
    "                text[i+1] = line + \" \" + next_line\n",
    "            text.pop(i)\n",
    "            continue\n",
    "        i += 1\n",
    "    \n",
    "    return\n",
    "\n",
    "# Removes unnecessary sections from the article text\n",
    "# This includes the \"Report a problem or mistake on this page\" and/or the \"Search for related information by keyword\" sections\n",
    "def remove_irrelevant_sections(text):\n",
    "    try:\n",
    "        start = text.index(\"Search for related information by keyword\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            start = text.index(\"Report a problem or mistake on this page\")\n",
    "        except ValueError:\n",
    "            return\n",
    "        \n",
    "    end = text.index(\"Date modified:\")\n",
    "    del text[start: end]\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function (generating function) to clean up all articles in a given department (or simply a list of articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans up all the articles in the given list of articles. \n",
    "# Note that this function assumes the incoming article objects have the same format as the raw articles \n",
    "# stored on MongoDB. That is, the full text is stored as a list of strings for each article object\n",
    "# and can be accessed from the article object using \"article_name['details']['fulltext']\"\n",
    "# This function creates a deep copy of each article and cleans the copy, so that original articles are \n",
    "# not affected. \n",
    "# The cleaned articles are then yielded one at a time.\n",
    "# The list name can also be provided for print statments which indicate the progress of the function\n",
    "# (this is useful for cleaning the articles for several different departments)\n",
    "# input: list of articles, string\n",
    "def cleanup_articles(list_of_articles, list_name = None):\n",
    "    \n",
    "    if list_name is not None:\n",
    "        print(\"Cleaning all\", len(list_of_articles), \"articles in\", list_name, \"...\", end=\" \")\n",
    "    \n",
    "    # copy and clean each article in the list\n",
    "    for i in range(0, len(list_of_articles)):\n",
    "        # create a deep copy so as to not affect the original list of articles\n",
    "        article = copy.deepcopy(list_of_articles[i])\n",
    "        text = article['details']['fulltext']\n",
    "        text = cleanup_text(text)\n",
    "        yield article\n",
    "    \n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the department names from the Excel sheet (*Departments and Programs.xlsx*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the department names\n",
    "dept_names = pd.read_excel(\"./excel_sheets/Departments and Programs.xlsx\", sheet_name = \"Departments\")['Department'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the database for the articles for each department, and clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning all 709 articles in Agriculture and Agri-Food Canada ... Success!\n",
      "Cleaning all 1031 articles in Atlantic Canada Opportunities Agency ... Success!\n",
      "Cleaning all 807 articles in Canada Economic Development for Quebec Regions ... Success!\n",
      "Cleaning all 1045 articles in Canadian Heritage ... Success!\n",
      "Cleaning all 177 articles in Canadian Institutes of Health Research ... Success!\n",
      "Cleaning all 125 articles in Canadian Northern Economic Development Agency ... Success!\n",
      "Cleaning all 73 articles in Canadian Space Agency ... Success!\n",
      "Cleaning all 738 articles in Environment and Climate Change Canada ... Success!\n",
      "Cleaning all 224 articles in Federal Economic Development Agency for Southern Ontario ... Success!\n",
      "Cleaning all 620 articles in Fisheries and Oceans Canada ... Success!\n",
      "Cleaning all 1509 articles in Global Affairs Canada ... Success!\n",
      "Cleaning all 980 articles in Innovation, Science and Economic Development Canada ... Success!\n",
      "Cleaning all 1374 articles in National Defence ... Success!\n",
      "Cleaning all 102 articles in National Research Council Canada ... Success!\n",
      "Cleaning all 200 articles in Natural Resources Canada ... Success!\n",
      "Cleaning all 1 articles in Natural Sciences and Engineering Research Council ... Success!\n",
      "Cleaning all 174 articles in Public Services and Procurement Canada ... Success!\n",
      "Cleaning all 400 articles in Western Economic Diversification Canada ... Success!\n"
     ]
    }
   ],
   "source": [
    "# create a list of cleaned articles\n",
    "cleaned_articles = list()\n",
    "\n",
    "# query the database for the articles for each department, and clean them up\n",
    "for dept in dept_names:\n",
    "    \n",
    "    dept_articles = list(records.find({'department':dept}))\n",
    "    \n",
    "    for article in cleanup_articles(dept_articles, dept):\n",
    "        cleaned_articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles found: 10289\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of articles found:\", len(cleaned_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned articles to the *cleaned_articles.json* file in the **data** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function needed to convert BSON into JSON\n",
    "from bson.json_util import dumps\n",
    "\n",
    "# create the dictionary to hold the list of articles\n",
    "data = {}\n",
    "data['cleaned_articles'] = []\n",
    "\n",
    "# add the articles to the dictionary\n",
    "for article in cleaned_articles:\n",
    "    data['cleaned_articles'].append(dumps(article))  # use dumps to convert BSON into JSON\n",
    "\n",
    "# save to the file (note, this re-writes the entire file)\n",
    "with open('./data/cleaned_articles.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
